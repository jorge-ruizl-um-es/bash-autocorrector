Cosas por hacer ahora mismo:

- Probar el programa con examen-resuelto-2024, pasándole los casos de prueba nuevos incluidos (NUM_BLOQUES = 4).
    - HECHO 

- Meter las salidas en una carpeta llamada "outputs" en el directorio actual, con los CSV que generemos con las correcciones.
    - HECHO

- Crear un nuevo CSV en el que, para cada alumno, doy las puntuaciones por bloques (idea: meter columnas del estilo B1-BLANK, B1-WRONG_COMMAND, etc.)
    - HECHO

- Crear una carpeta "answers" en la que generemos 5 CSV, uno por cada bloque. Las columnas serán los comandos sacados de examen-resuelto para cada bloque. En cada fila (alumno), el valor de la columna será su respuesta a esa pregunta, o "" si está en blanco.
    - HECHO

---

- Pensar la forma de hacer que las correcciones (estados de un comando) no sean disjuntas. Un comando puede tener a la vez mal la opción y el argumento, y quizás habría que contarlo como tal.

    - Podemos hacer que se generen otros 5 CSV complementarios a los que tienen los comandos en detalle, que tengan la misma estructura de, para cada alumno, dar cada pregunta
        - HECHO

    - La función correction2 del módulo correct_exam debería devolver una lista de estados para cada comando que le pasemos.
        - HECHO

    - El diccionario construido en detalle en la función compare_commands almacenará la lista de estados.
    - El diccionario de scores será actualizado con cada una de las salidas de CommandStatus de la lista de correction2.

    - El conteo general podemos verlo entonces como un conteo de errores del alumno, en vez de como una clasificación de preguntas en distintos estados. Sólo se contarán como CORRECT aquellas que estén perfectas. 

- MEJORA DE RENDIMIENTO: Aplicar corrección general para los bloques 1-3; pero mirar los bloques 4, 5 y 6 e implementar funciones de corrección más específicas a los comandos de estos (esto parece menos prioritario) --> gcc, objdump...

- Más complicado: Creo que una cosa muy interesante para mejorarlo es que se le pueda pasar al script de corrección una opción que le indique que se está ejecutando en el mismo entorno del examen (particularmente, el sistema de ficheros que se percibe), y en tal caso, que una de las acciones que lleve a cabo el script sea comparar la salida del comando entregado con la salida del comando resuelto (quizás ordenando ambas previamente, para evitar discrepancias por el orden).
    - Esto puede hacerse en un módulo a parte, en el que desarrollemos una función que sea capaz de ejecutar el notebook del alumno y el de respuestas, y quizás hacer un diff o algo para comparar los outputs.
    - Habría que probarlo en el entorno de jupyter.

